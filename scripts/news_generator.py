import os
import re
import json
import datetime
from google import genai
from google.genai import types

# ================= é…ç½®åŒº =================
# ä» GitHub Secrets è·å– API Key
API_KEY = os.environ.get("GEMINI_API_KEY")
HTML_FILE_PATH = "index.html"

def get_current_week_info():
    """è·å–å½“å‰çš„æ—¥æœŸã€å¹´ä»½å’Œå‘¨æ•°"""
    today = datetime.date.today()
    # ISO å‘¨å†
    year, week_num, _ = today.isocalendar()
    return {
        "vol": f"VOL.{week_num:02d}",
        "week": f"Week {week_num:02d}",
        "date": today.strftime("%Y.%m.%d"),
        "year": str(year)
    }

def extract_json_from_text(text):
    """å°è¯•ä»æ··åˆæ–‡æœ¬ä¸­æå– JSON åˆ—è¡¨"""
    try:
        # 1. å°è¯•ç›´æ¥è§£æ
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    try:
        # 2. å°è¯•æå– Markdown ä»£ç å— ```json ... ```
        match = re.search(r'```json\s*(\[[\s\S]*?\])\s*```', text)
        if match:
            return json.loads(match.group(1))
        
        # 3. å°è¯•å¯»æ‰¾æœ€å¤–å±‚çš„æ–¹æ‹¬å· []
        start = text.find('[')
        end = text.rfind(']')
        if start != -1 and end != -1:
            json_str = text[start:end+1]
            return json.loads(json_str)
            
    except Exception as e:
        print(f"JSON æå–å¤±è´¥: {e}")
    
    return None

def generate_news_content():
    """è°ƒç”¨ Gemini API ç”Ÿæˆæ–°é—»æ•°æ® (ä½¿ç”¨æ–°ç‰ˆ google-genai SDK)"""
    if not API_KEY:
        raise ValueError("âŒ é”™è¯¯: æœªæ‰¾åˆ° GEMINI_API_KEY ç¯å¢ƒå˜é‡ã€‚è¯·åœ¨ GitHub Secrets æˆ–æœ¬åœ°ç¯å¢ƒå˜é‡ä¸­é…ç½®ã€‚")

    print(f"ğŸš€ æ­£åœ¨è¿æ¥ Gemini API (key length: {len(API_KEY)})...")
    
    # åˆå§‹åŒ–æ–°ç‰ˆå®¢æˆ·ç«¯
    client = genai.Client(api_key=API_KEY)

    # æ ¸å¿ƒ Prompt
    prompt = f"""
    ä½ æ˜¯ä¸€åä¸¥æ ¼éµå®ˆäº‹å®æ ¸æŸ¥ä¸ä¿¡æ¯æº¯æºè§„èŒƒçš„å›½é™…æ•™è‚²ä¸å…¨çƒèµ„è®¯ç¼–è¾‘ã€‚
    ç°åœ¨æ˜¯ {datetime.date.today().strftime("%Yå¹´%mæœˆ%dæ—¥")}ã€‚

    è¯·ä½ å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼Œå¹¶ã€åªè¾“å‡ºæœ€ç»ˆèµ„è®¯å†…å®¹æœ¬èº«ã€‘ã€‚ä¸ºäº†è®©ç¨‹åºèƒ½å¤Ÿå¤„ç†ï¼Œ**è¯·åŠ¡å¿…å°†ç»“æœå°è£…ä¸º JSON æ ¼å¼**ï¼ˆå…·ä½“æ ¼å¼è§ä¸‹æ–¹ï¼‰ã€‚

    ã€æ—¶é—´èŒƒå›´ã€‘
    ä»…æ£€ç´¢å¹¶æ•´ç†ã€æœ€è¿‘ 7 å¤©å†…é¦–æ¬¡å‘å¸ƒã€‘çš„ä¿¡æ¯ã€‚

    ã€ä¿¡æ¯æ¥æºè¦æ±‚ã€‘
    ä»…é™ä»¥ä¸‹æ¥æºï¼š
    - å¾®åšã€å¾®ä¿¡å…¬ä¼—å·ï¼ˆé«˜æ ¡ / æƒå¨åª’ä½“å®˜æ–¹è´¦å·ï¼‰
    - å›½å†…å¤–æƒå¨æ–°é—»åª’ä½“å®˜æ–¹è´¦å·ï¼ˆå¦‚ BBC / Reuters / FT / NYTimes ç­‰ï¼‰
    - æµ·å¤–å¤§å­¦å®˜æ–¹ ç½‘ç«™/Instagram / X(Twitter) / Facebook è´¦å·

    ã€æ˜ç¡®æ’é™¤ã€‘
    - ä»»ä½•æ•™åŸ¹ã€ç•™å­¦ä¸­ä»‹ã€å•†ä¸šæ¨å¹¿å†…å®¹ï¼ˆå¦‚æ–°ä¸œæ–¹ã€å¯å¾·ã€IDP ç­‰ï¼‰
    - äºŒæ¬¡è½¬è½½ã€è§‚ç‚¹è¯„è®ºã€æœªç»è¯å®çš„ä¿¡æ¯

    ã€å†…å®¹ä¸»é¢˜èŒƒå›´ã€‘
    è¯·å›´ç»•ä»¥ä¸‹å…­ç±»èµ„è®¯è¿›è¡Œç­›é€‰ä¸æ•´ç†ï¼ˆæ¯ä¸ªæ¿å—ç²¾é€‰ 5 æ¡ï¼Œæ€»è®¡ 30 æ¡ï¼‰ï¼š
    1. global (ç¤¾ä¼šçƒ­ç‚¹ / å›½é™…æ–°é—»)
    2. education (æµ·å†…å¤–çƒ­ç‚¹æ•™è‚²ç±»æ–°é—»)
    3. university (ä¸–ç•Œé¡¶å°–é™¢æ ¡å®˜æ–¹åŠ¨æ€)
    4. design (æ•°å­—åª’ä½“ / æ¸¸æˆåŠ¨ç”» / äº¤äº’è®¾è®¡ / æ™ºèƒ½å·¥ç¨‹ / å»ºç­‘ / æ™¯è§‚ / åŸå¸‚ç›¸å…³ç±»/ å·¥ä¸šäº§å“ç±» / è§†è§‰ä¼ è¾¾/äº¤å‰å­¦ç§‘ç­‰ç›¸å…³ä¸“ä¸šçš„æœ¬ç§‘/ç¡•å£«ç•™å­¦ç”³è¯·è¶‹åŠ¿ã€å®˜æ–¹è¯¾ç¨‹ç»“æ„æˆ–åŸ¹å…»æ–¹å‘å˜åŒ–)
    5. summer (Summer School / æš‘æœŸç§‘ç ”é¡¹ç›®ä¿¡æ¯)
    6. competitions (æˆªæ­¢æ—¶é—´åœ¨æœªæ¥çš„å›½é™…æƒå¨ç«èµ›)

    ã€ç­›é€‰ä¸æ’åºè§„åˆ™ã€‘
    - æ€»æ•°ï¼šä¸¥æ ¼ä¿ç•™ 30 æ¡
    - æŒ‰â€œç¤¾äº¤åª’ä½“è®¨è®ºçƒ­åº¦ + æƒå¨æ€§â€ç»¼åˆæ’åº
    - åŒä¸€äº‹ä»¶åªä¿ç•™ 1 æ¡

    ã€æ ¼å¼è¦æ±‚è½¬æ¢ã€‘
    è¯·å°†ä½ ä½œä¸ºç¼–è¾‘æ•´ç†å¥½çš„å†…å®¹æ˜ å°„åˆ°ä»¥ä¸‹ JSON ç»“æ„ä¸­ã€‚
    å¯¹äº content å­—æ®µï¼Œè¯·ä¸¥æ ¼æ‰§è¡Œï¼š**å…³é”®è¯ / å…³é”®è¯**ï¼šä¸¤è¡Œæ–‡å­—æ¦‚è¿°äº‹ä»¶æ ¸å¿ƒä¿¡æ¯ã€‚
    å¯¹äº source å’Œ date å­—æ®µï¼Œè¯·æå–æ‹¬å·å†…çš„ï¼ˆå‘å¸ƒæ–¹ â€“ æ¨é€æ—¶é—´ï¼‰ã€‚
    
    **å…³é”®è¦æ±‚ï¼š**
    1. `url`: å¿…é¡»æ˜¯çœŸå®çš„ã€å¯è®¿é—®çš„åŸå§‹æ–°é—»é“¾æ¥ï¼ˆä»¥ http å¼€å¤´ï¼‰ï¼Œä¸èƒ½ç•™ç©ºã€‚
    2. `image`: è¯·å°è¯•å¯»æ‰¾æ¯æ¡æ–°é—»çš„ç›¸å…³å›¾ç‰‡ URLã€‚
    3. `analysis`: é’ˆå¯¹è¯¥æ–°é—»ï¼Œå†™ä¸€æ®µç®€çŸ­çŠ€åˆ©çš„â€œä¸“å®¶ç‚¹è¯„â€ï¼ˆ2å¥è¯ï¼‰ï¼Œé’ˆå¯¹å­¦ç”Ÿ/å®¶é•¿ï¼Œåˆ†æå…¶å¯¹ç”³è¯·æˆ–æœªæ¥çš„å½±å“ã€‚

    JSON è¾“å‡ºç¤ºä¾‹ï¼š
    [
        {{
            "id": "global",
            "items": [
                {{
                    "title": "å…³é”®è¯ (Emoji + ä¸­æ–‡)", 
                    "content": "**å…³é”®è¯ / å…³é”®è¯**ï¼šä¸¤è¡Œæ–‡å­—æ¦‚è¿°äº‹ä»¶æ ¸å¿ƒä¿¡æ¯ã€‚", 
                    "source": "å‘å¸ƒæ–¹", 
                    "date": "MM.DD", 
                    "image": "https://example.com/news-image.jpg",
                    "tags": ["Tag1", "Tag2"],
                    "url": "https://www.bbc.com/news/example-story",
                    "fullContent": "<p>è¿™é‡Œå†™ä¸€æ®µè¯¦ç»†æŠ¥é“ï¼ˆçº¦150å­—ï¼‰ï¼Œæ”¯æŒHTMLæ ‡ç­¾ã€‚</p>",
                    "analysis": "è¿™é‡Œå†™ä¸“å®¶ç‚¹è¯„..."
                }}
            ]
        }},
        ... å…¶ä»–æ¿å—
    ]
    """

    print("ğŸ” æ­£åœ¨è°ƒç”¨ Gemini API è¿›è¡Œä¸¥æ ¼ç­›é€‰ä¸ç”Ÿæˆ... (Target: 30 items)")
    try:
        # ä½¿ç”¨æ–°ç‰ˆ SDK è°ƒç”¨æ–¹æ³•
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp',
            contents=prompt,
            config=types.GenerateContentConfig(
                tools=[types.Tool(google_search=types.GoogleSearch())] # æ–°ç‰ˆå·¥å…·å®šä¹‰æ–¹å¼
            )
        )
    except Exception as e:
        print(f"âŒ API è°ƒç”¨å¤±è´¥: {e}")
        raise

    print("âœ… API å“åº”æˆåŠŸï¼Œæ­£åœ¨è§£æ JSON...")
    
    # æ–°ç‰ˆ SDK çš„ response.text ç›´æ¥å¯ç”¨
    news_data = extract_json_from_text(response.text)
    
    if not news_data:
        print("âŒ é”™è¯¯: æ— æ³•ä» AI å“åº”ä¸­æå–æœ‰æ•ˆçš„ JSON æ•°æ®ã€‚")
        print("ğŸ” åŸå§‹å“åº”ç‰‡æ®µ (å‰500å­—ç¬¦):")
        try:
            print(response.text[:500] + "...")
        except:
            print("æ— æ³•æ‰“å°å“åº”å†…å®¹")
        raise ValueError("Invalid JSON response from AI")
        
    return news_data

def update_html_file(news_data, week_info):
    """è¯»å– index.html å¹¶æ›´æ–° JS æ•°æ®éƒ¨åˆ†"""
    if not os.path.exists(HTML_FILE_PATH):
        raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ° {HTML_FILE_PATH} æ–‡ä»¶ï¼Œè¯·ç¡®ä¿è„šæœ¬åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œã€‚")

    with open(HTML_FILE_PATH, 'r', encoding='utf-8') as f:
        content = f.read()

    # 1. æ›´æ–° ISSUE_CONFIG
    new_config_str = f"""const ISSUE_CONFIG = {{
            vol: "{week_info['vol']}",           
            week: "{week_info['week']}",         
            date: "{week_info['date']}",      
            year: "{week_info['year']}"
        }};"""
    
    content = re.sub(
        r'const\s+ISSUE_CONFIG\s*=\s*\{[\s\S]*?\};', 
        new_config_str, 
        content
    )

    # 2. æ›´æ–° SECTIONS
    static_props = {
        'global': {'subtitle': 'æ”¿ç­–é£å‘ä¸å¤§äº‹ä»¶', 'bgColor': 'bg-[#FF4D00]', 'textColor': 'text-white'},
        'education': {'subtitle': 'ç•™å­¦è¶‹åŠ¿ä¸å­¦è´¹å˜åŠ¨', 'bgColor': 'bg-[#CCFF00]', 'textColor': 'text-black'},
        'university': {'subtitle': 'æ‹›ç”Ÿç®€ç« ä¸æˆªæ­¢æ—¥', 'bgColor': 'bg-[#0047FF]', 'textColor': 'text-white'},
        'design': {'subtitle': 'é»‘ç§‘æŠ€ä¸è®¾è®¡é£å£', 'bgColor': 'bg-[#FF00FF]', 'textColor': 'text-white'},
        'summer': {'subtitle': 'èƒŒæ™¯æå‡æœºä¼š', 'bgColor': 'bg-[#00FF94]', 'textColor': 'text-black'},
        'competitions': {'subtitle': 'é«˜å«é‡‘é‡èµ›äº‹', 'bgColor': 'bg-[#1A1A1A]', 'textColor': 'text-white'}
    }
    
    titles = {
        'global': 'GLOBAL NEWS', 'education': 'EDUCATION',
        'university': 'UNIVERSITY', 'design': 'TECH & DESIGN',
        'summer': 'LABS', 'competitions': 'COMPETITIONS'
    }

    js_sections_str = "const SECTIONS = [\n"
    
    global_id_counter = 1

    for section_data in news_data:
        sec_id = section_data['id']
        props = static_props.get(sec_id, {})
        
        display_title = titles.get(sec_id, sec_id.upper())
        
        js_sections_str += "            {\n"
        js_sections_str += f"                id: '{sec_id}',\n"
        js_sections_str += f"                title: '{display_title}',\n"
        js_sections_str += f"                subtitle: '{props.get('subtitle', '')}',\n"
        js_sections_str += f"                bgColor: '{props.get('bgColor', '')}',\n"
        js_sections_str += f"                textColor: '{props.get('textColor', '')}',\n"
        js_sections_str += "                items: [\n"
        
        items = section_data.get('items', [])
        for item in items:
            current_id = global_id_counter
            global_id_counter += 1

            clean_content = str(item.get('fullContent', '')).replace('\n', '').replace('"', '\\"').replace("'", "\\'")
            clean_summary = str(item.get('content', '')).replace('"', '\\"').replace("'", "\\'")
            clean_title = str(item.get('title', '')).replace('"', '\\"').replace("'", "\\'")
            clean_image = str(item.get('image', '')).replace('"', '\\"')
            clean_analysis = str(item.get('analysis', '')).replace('"', '\\"').replace("'", "\\'")
            clean_source = str(item.get('source', 'RAC News'))
            clean_date = str(item.get('date', ''))
            clean_url = str(item.get('url', '#'))
            
            tags = item.get('tags', [])
            tags_str = json.dumps(tags, ensure_ascii=False) if isinstance(tags, list) else "[]"
            
            js_sections_str += "                    {\n"
            js_sections_str += f"                        id: {current_id},\n"
            js_sections_str += f"                        title: \"{clean_title}\",\n"
            js_sections_str += f"                        content: \"{clean_summary}\",\n"
            js_sections_str += f"                        source: \"{clean_source}\",\n"
            js_sections_str += f"                        date: \"{clean_date}\",\n"
            js_sections_str += f"                        image: \"{clean_image}\",\n"
            js_sections_str += f"                        tags: {tags_str},\n"
            js_sections_str += f"                        url: \"{clean_url}\",\n"
            js_sections_str += f"                        fullContent: \"{clean_content}\",\n"
            js_sections_str += f"                        analysis: \"{clean_analysis}\"\n"
            js_sections_str += "                    },\n"
        
        js_sections_str += "                ]\n"
        js_sections_str += "            },\n"

    js_sections_str += "        ];"

    content = re.sub(
        r'const\s+SECTIONS\s*=\s*\[([\s\S]*?)\];', 
        js_sections_str, 
        content
    )

    with open(HTML_FILE_PATH, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"âœ… æˆåŠŸæ›´æ–° {HTML_FILE_PATH}ï¼ç‰ˆæœ¬: {week_info['vol']} ({week_info['date']})")

if __name__ == "__main__":
    try:
        print("ğŸ¬ å¼€å§‹æ‰§è¡Œå‘¨æ›´ä»»åŠ¡...")
        week_info = get_current_week_info()
        print(f"ğŸ“… ç›®æ ‡ç‰ˆæœ¬: {week_info['vol']} ({week_info['date']})")
        
        news_data = generate_news_content()
        update_html_file(news_data, week_info)
        
        print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆã€‚")
    except Exception as e:
        print(f"âŒ ä»»åŠ¡å¤±è´¥: {e}")
        exit(1)
